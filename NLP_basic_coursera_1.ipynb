{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_basic_coursera_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ertn9I7Yf73x",
        "colab_type": "text"
      },
      "source": [
        "**link for this tutorial:-**\n",
        "https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/Sydkf/notebook-for-lesson-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekrL4IoU5F25",
        "colab_type": "code",
        "outputId": "569ceef5-fb5a-4bf2-8866-18f128e63dfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (2.0.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (2.0.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (42.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.10.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.7)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8oA0aJwSs9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxWYuSNFTL9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT_oW-CoTcah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "             'I love my dog.',\n",
        "             'I love my cat',\n",
        "             'you love my dog!',\n",
        "             'Do you think my dog is amazing?'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwKAnltnVauU",
        "colab_type": "text"
      },
      "source": [
        "## tokenize means to give tokens or specific number to each words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmVl6bWTT-DX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100,oov_token='<OOV>')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Q9YhEDUVIM",
        "colab_type": "text"
      },
      "source": [
        "num_words highest repeated words(frequency basis) linxa. hamro case ma 100 repeated words. Yaha hamro total words 100 xaina so sabai words atauxa, \n",
        "oov_token le chai jun word ahile specify gariyeko xaina ra tyo unspecified words yadi test ma aauxa vane tyo words harulai euta index dinxa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWqyrcmqfVED",
        "colab_type": "text"
      },
      "source": [
        "### OOV means Out Of Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waWa9kbCUSnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrapqFbqVq4-",
        "colab_type": "text"
      },
      "source": [
        "word_index le chai dictionary return garxa tokenized words ko"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwbHxUNuVMF3",
        "colab_type": "code",
        "outputId": "68baaaa8-98ba-49c9-902b-60101eee14e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "word_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<OOV>': 1,\n",
              " 'amazing': 11,\n",
              " 'cat': 7,\n",
              " 'do': 8,\n",
              " 'dog': 4,\n",
              " 'i': 5,\n",
              " 'is': 10,\n",
              " 'love': 3,\n",
              " 'my': 2,\n",
              " 'think': 9,\n",
              " 'you': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD8YMhLkV3G7",
        "colab_type": "text"
      },
      "source": [
        "each words lai euta euta number diyo, OOV lai 1, cat lai 7 and so on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6dim5Y3WHD4",
        "colab_type": "text"
      },
      "source": [
        "### text_to_sequences le chai each sentences ko words ko index ko list banauxa "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gszDWrm-Vz2v",
        "colab_type": "code",
        "outputId": "309dfcac-fb92-4a3a-a7fc-71fe3be6de42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAMYTFnLW7tu",
        "colab_type": "text"
      },
      "source": [
        "tyo 4 ota sub_lists chai 4 ota sentences ko words ko ho\n",
        "example:-[5, 3, 2, 4] = 5-> 'i', 3-> 'love', 2->'my' , 4->'dog' (i love my dog)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCw6UlzZVQ4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIibH3V7XzUr",
        "colab_type": "text"
      },
      "source": [
        "## pad_sequences le chai array banauxa mathi ko sequences ko"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXp8EiQDX5mO",
        "colab_type": "code",
        "outputId": "3c07f36c-3e3a-4298-894e-c5ead835ff78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "padding = pad_sequences(sequences= sequences,maxlen= 8)\n",
        "padding"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  5,  3,  2,  4],\n",
              "       [ 0,  0,  0,  0,  5,  3,  2,  7],\n",
              "       [ 0,  0,  0,  0,  6,  3,  2,  4],\n",
              "       [ 0,  8,  6,  9,  2,  4, 10, 11]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEgWceP3YSJ5",
        "colab_type": "text"
      },
      "source": [
        "compare the sequences with this padding and you will know what has happened here, maxlen = 8 means there will only be 8 vertical columns in the padded array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yISwmBPUY0ed",
        "colab_type": "text"
      },
      "source": [
        "### trying some other params in  the padding, just for educational purpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmJ865ZXYOhU",
        "colab_type": "code",
        "outputId": "282c0bf0-79f9-49af-f674-a3899c89daae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "padding_1 = pad_sequences(sequences=sequences,maxlen=4)\n",
        "padding_1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5,  3,  2,  4],\n",
              "       [ 5,  3,  2,  7],\n",
              "       [ 6,  3,  2,  4],\n",
              "       [ 2,  4, 10, 11]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMryNauMZJ_o",
        "colab_type": "text"
      },
      "source": [
        "baam, only 4 vertical columns, and if the sequence list has more than 4 words then padding will only take last four words, as we can see in the last horizontal row of paddin_1. It completely ignored the previous elems of the sequences i.e 8,6,9 and took only 2,4,10,11\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpAPfby6aPJL",
        "colab_type": "text"
      },
      "source": [
        "watch coursera video for more info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18TgqSi9aZaE",
        "colab_type": "text"
      },
      "source": [
        "## some test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru_a887wZGpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentences = [\n",
        "             'i really love my dog',\n",
        "             'my dog loves cookies'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePy3HH8fakVG",
        "colab_type": "code",
        "outputId": "74cffad5-eec3-4c5d-908a-0489b4b52af9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 1, 3, 2, 4], [2, 4, 1, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcH_0RD8bA5R",
        "colab_type": "text"
      },
      "source": [
        "as we can see here in [5, 1, 3, 2, 4], as per the tokenized list, 5->'i', 1->'OOV', 3->'love' and so on.\n",
        " <br>Here '1'(OOV) is assigned to 'really' as 'really' was not the part of training data which we tokenized "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0TMje0Na1fB",
        "colab_type": "code",
        "outputId": "6583be89-4e6f-4105-a314-38d809ffcf46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "test_padding = pad_sequences(sequences=test_sequences,maxlen = 10)\n",
        "test_padding"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 5, 1, 3, 2, 4],\n",
              "       [0, 0, 0, 0, 0, 0, 2, 4, 1, 1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt5rG1KzcGXx",
        "colab_type": "code",
        "outputId": "bb7fbb5e-2bc5-4d33-c3f6-f4f110159fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_padding.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MLA5CflhusB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}